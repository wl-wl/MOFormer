#/tmp/pycharm_project_988/example/test_2/sample_other49.txt
import numpy as np
from transformers import AutoTokenizer,AutoModelForSequenceClassification
from transformers import set_seed
import torch
import torch.nn as nn
import warnings
# warnings.filterwarnings('ignore')
# warnings.simplefilter()
# warnings.filterwarnings("ignore", message="Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForSequenceClassification")
set_seed(4)
device = torch.device("cuda:2" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")  # 打印正在使用的设备

if device.type == 'cuda':
    print(torch.cuda.get_device_name(2))
model_checkpoint = "facebook/esm2_t6_8M_UR50D"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def AMP(file):
    test_sequences = file
    max_len = 60
    test_data = tokenizer(test_sequences, max_length=max_len, padding="max_length",truncation=True, return_tensors='pt')
    test_data = {key: value.to(device) for key, value in test_data.items()}
    class MyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.bert = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=320)
            self.bn1 = nn.BatchNorm1d(256)
            self.bn2 = nn.BatchNorm1d(128)
            self.bn3 = nn.BatchNorm1d(64)
            self.relu = nn.ReLU()
            self.fc1 = nn.Linear(320,256)
            self.fc2 = nn.Linear(256,128)
            self.fc3 = nn.Linear(128,64)
            self.output_layer = nn.Linear(64,2)
            self.dropout = nn.Dropout(0)

        def forward(self,x):
            with torch.no_grad():
                bert_output = self.bert(input_ids=x['input_ids'].to(device),attention_mask=x['attention_mask'].to(device))
            output_feature = self.dropout(bert_output["logits"])
            output_feature = self.relu(self.bn1(self.fc1(output_feature)))
            output_feature = self.relu(self.bn2(self.fc2(output_feature)))
            output_feature = self.relu(self.bn3(self.fc3(output_feature)))
            output_feature = self.output_layer(output_feature)
            return torch.softmax(output_feature,dim=1)

    model = MyModel()
    model.load_state_dict(torch.load("best_toxi_model.pth", map_location=device),strict=False)
    model = model.to(device)
    model.eval()
    out_probability = []
    with torch.no_grad():
        predict = model(test_data)
        # out_probability.append(predict[1])
    #     out_probability.extend(np.max(np.array(predict.cpu()),axis=1).tolist())
    #     test_argmax = np.argmax(predict.cpu(), axis=1).tolist()
    # id2str = {0:"non-hemo-AMP", 1:"hemo-AMP"}
    return  predict[0][1]

def p3():
    B=[]
    with open('peptide_train.txt','r') as f:
        for line in f:
            # print(line)
            if len(line)<10:
                continue
            str=line[0]
            for j in line:
                str+=' '
                str+=j.upper()
            b = AMP(str)
            B.append(b)
    return B
print(p3())

#
# file = "k w i p w m q t l w f".upper()  # Your seqs
# a,b = AMP(file)
# print(a,b)
#
# file = "qgcpgyqdhcwkvclcngkggpyghlgglvgt".upper()  # Your seqs
# a,b = AMP(file)
# print(a,b)

# [tensor([[1.3186]], device='cuda:0'), tensor([[0.6081]], device='cuda:0'), tensor([[0.2295]], device='cuda:0'), tensor([[-0.0997]], device='cuda:0'), tensor([[1.3044]], device='cuda:0'), tensor([[-0.0917]], device='cuda:0'), tensor([[0.4362]], device='cuda:0'), tensor([[0.8505]], device='cuda:0'), tensor([[-0.0956]], device='cuda:0'), tensor([[0.1513]], device='cuda:0'), tensor([[0.4557]], device='cuda:0'), tensor([[0.2148]], device='cuda:0'), tensor([[0.3022]], device='cuda:0'), tensor([[-0.1147]], device='cuda:0'), tensor([[-0.1692]], device='cuda:0'), tensor([[0.2218]], device='cuda:0'), tensor([[0.0029]], device='cuda:0'), tensor([[-0.2491]], device='cuda:0'), tensor([[-0.1814]], device='cuda:0'), tensor([[-0.0401]], device='cuda:0'), tensor([[0.9096]], device='cuda:0'), tensor([[0.3619]], device='cuda:0'), tensor([[0.7774]], device='cuda:0'), tensor([[0.3593]], device='cuda:0'), tensor([[0.1483]], device='cuda:0'), tensor([[0.1427]], device='cuda:0'), tensor([[0.6551]], device='cuda:0'), tensor([[-0.2920]], device='cuda:0'), tensor([[0.3533]], device='cuda:0'), tensor([[1.6606]], device='cuda:0'), tensor([[1.6606]], device='cuda:0'), tensor([[0.1590]], device='cuda:0'), tensor([[1.3798]], device='cuda:0'), tensor([[1.3798]], device='cuda:0'), tensor([[2.5270]], device='cuda:0'), tensor([[-0.6540]], device='cuda:0'), tensor([[1.0349]], device='cuda:0'), tensor([[0.1817]], device='cuda:0'), tensor([[0.1173]], device='cuda:0'), tensor([[-0.2140]], device='cuda:0'), tensor([[0.8699]], device='cuda:0'), tensor([[0.3421]], device='cuda:0'), tensor([[0.3259]], device='cuda:0'), tensor([[0.7128]], device='cuda:0'), tensor([[0.4840]], device='cuda:0'), tensor([[0.5399]], device='cuda:0'), tensor([[0.0641]], device='cuda:0'), tensor([[1.3671]], device='cuda:0'), tensor([[0.5627]], device='cuda:0'), tensor([[-0.2082]], device='cuda:0'), tensor([[0.3314]], device='cuda:0'), tensor([[0.5517]], device='cuda:0'), tensor([[-0.1391]], device='cuda:0'), tensor([[-0.1779]], device='cuda:0'), tensor([[-0.3592]], device='cuda:0'), tensor([[-0.2703]], device='cuda:0'), tensor([[0.6734]], device='cuda:0'), tensor([[0.2076]], device='cuda:0'), tensor([[0.0558]], device='cuda:0'), tensor([[1.4909]], device='cuda:0'), tensor([[0.0760]], device='cuda:0'), tensor([[-0.2587]], device='cuda:0'), tensor([[0.5015]], device='cuda:0'), tensor([[-0.3423]], device='cuda:0'), tensor([[-0.1590]], device='cuda:0'), tensor([[1.7934]], device='cuda:0'), tensor([[0.6110]], device='cuda:0'), tensor([[0.2874]], device='cuda:0'), tensor([[0.0717]], device='cuda:0'), tensor([[-0.3371]], device='cuda:0'), tensor([[0.1956]], device='cuda:0'), tensor([[0.4253]], device='cuda:0'), tensor([[0.0286]], device='cuda:0'), tensor([[-0.1162]], device='cuda:0'), tensor([[0.5701]], device='cuda:0'), tensor([[0.2984]], device='cuda:0'), tensor([[0.1463]], device='cuda:0'), tensor([[0.2902]], device='cuda:0'), tensor([[0.1382]], device='cuda:0'), tensor([[0.4494]], device='cuda:0'), tensor([[0.4521]], device='cuda:0'), tensor([[1.4321]], device='cuda:0'), tensor([[1.4321]], device='cuda:0'), tensor([[1.8621]], device='cuda:0'), tensor([[1.8621]], device='cuda:0'), tensor([[0.2266]], device='cuda:0'), tensor([[0.2266]], device='cuda:0'), tensor([[0.5938]], device='cuda:0'), tensor([[-0.0919]], device='cuda:0'), tensor([[1.7414]], device='cuda:0'), tensor([[1.7414]], device='cuda:0'), tensor([[1.1039]], device='cuda:0'), tensor([[0.1777]], device='cuda:0'), tensor([[-0.0960]], device='cuda:0'), tensor([[-0.1847]], device='cuda:0'), tensor([[0.1404]], device='cuda:0'), tensor([[-0.0036]], device='cuda:0'), tensor([[-0.1671]], device='cuda:0'), tensor([[0.6795]], device='cuda:0'), tensor([[0.5086]], device='cuda:0'), tensor([[1.2688]], device='cuda:0'), tensor([[0.6746]], device='cuda:0'), tensor([[0.0723]], device='cuda:0'), tensor([[0.5672]], device='cuda:0'), tensor([[0.8911]], device='cuda:0'), tensor([[0.8281]], device='cuda:0'), tensor([[0.5895]], device='cuda:0'), tensor([[0.0566]], device='cuda:0'), tensor([[0.6386]], device='cuda:0'), tensor([[0.3802]], device='cuda:0'), tensor([[0.3802]], device='cuda:0'), tensor([[2.4111]], device='cuda:0'), tensor([[0.5439]], device='cuda:0'), tensor([[2.2928]], device='cuda:0'), tensor([[-0.3286]], device='cuda:0'), tensor([[0.3302]], device='cuda:0'), tensor([[0.7575]], device='cuda:0'), tensor([[0.1970]], device='cuda:0'), tensor([[0.4527]], device='cuda:0'), tensor([[1.3118]], device='cuda:0'), tensor([[0.2404]], device='cuda:0'), tensor([[0.0551]], device='cuda:0'), tensor([[-0.1971]], device='cuda:0'), tensor([[0.1267]], device='cuda:0'), tensor([[2.2372]], device='cuda:0'), tensor([[2.5463]], device='cuda:0'), tensor([[0.7000]], device='cuda:0'), tensor([[0.6766]], device='cuda:0'), tensor([[1.1123]], device='cuda:0'), tensor([[0.3917]], device='cuda:0'), tensor([[0.0471]], device='cuda:0'), tensor([[-0.1170]], device='cuda:0'), tensor([[-0.3295]], device='cuda:0'), tensor([[0.0420]], device='cuda:0'), tensor([[0.6380]], device='cuda:0'), tensor([[0.6059]], device='cuda:0'), tensor([[0.5632]], device='cuda:0'), tensor([[-0.0045]], device='cuda:0'), tensor([[0.9409]], device='cuda:0'), tensor([[0.0960]], device='cuda:0'), tensor([[-0.3531]], device='cuda:0'), tensor([[0.7886]], device='cuda:0'), tensor([[0.5593]], device='cuda:0'), tensor([[0.3791]], device='cuda:0'), tensor([[0.1640]], device='cuda:0'), tensor([[-0.3941]], device='cuda:0'), tensor([[0.5749]], device='cuda:0'), tensor([[-0.6244]], device='cuda:0'), tensor([[0.3276]], device='cuda:0'), tensor([[0.2095]], device='cuda:0'), tensor([[0.6634]], device='cuda:0'), tensor([[0.6987]], device='cuda:0'), tensor([[0.7508]], device='cuda:0'), tensor([[1.6268]], device='cuda:0'), tensor([[0.5203]], device='cuda:0'), tensor([[0.3087]], device='cuda:0'), tensor([[0.2405]], device='cuda:0'), tensor([[0.2405]], device='cuda:0'), tensor([[-0.3291]], device='cuda:0'), tensor([[0.4798]], device='cuda:0'), tensor([[-0.4106]], device='cuda:0'), tensor([[-0.0547]], device='cuda:0'), tensor([[0.5470]], device='cuda:0'), tensor([[0.6487]], device='cuda:0'), tensor([[-0.1300]], device='cuda:0'), tensor([[0.2264]], device='cuda:0'), tensor([[0.6050]], device='cuda:0'), tensor([[0.2219]], device='cuda:0'), tensor([[-0.0378]], device='cuda:0'), tensor([[0.3408]], device='cuda:0'), tensor([[0.1575]], device='cuda:0'), tensor([[0.1575]], device='cuda:0'), tensor([[0.4241]], device='cuda:0'), tensor([[0.2913]], device='cuda:0'), tensor([[0.3464]], device='cuda:0'), tensor([[-0.5375]], device='cuda:0'), tensor([[-0.2302]], device='cuda:0'), tensor([[0.7873]], device='cuda:0'), tensor([[0.8154]], device='cuda:0'), tensor([[-0.2951]], device='cuda:0'), tensor([[0.4736]], device='cuda:0'), tensor([[1.4888]], device='cuda:0'), tensor([[0.9985]], device='cuda:0'), tensor([[-0.4249]], device='cuda:0'), tensor([[0.1780]], device='cuda:0'), tensor([[-0.1150]], device='cuda:0'), tensor([[1.2808]], device='cuda:0'), tensor([[-0.3958]], device='cuda:0'), tensor([[0.4112]], device='cuda:0'), tensor([[1.4286]], device='cuda:0'), tensor([[0.1301]], device='cuda:0'), tensor([[-0.3126]], device='cuda:0'), tensor([[1.5508]], device='cuda:0'), tensor([[1.3996]], device='cuda:0'), tensor([[0.4722]], device='cuda:0'), tensor([[0.6970]], device='cuda:0'), tensor([[0.6970]], device='cuda:0'), tensor([[-0.0571]], device='cuda:0'), tensor([[0.5112]], device='cuda:0'), tensor([[0.5389]], device='cuda:0'), tensor([[1.3275]], device='cuda:0'), tensor([[1.8733]], device='cuda:0'), tensor([[0.4030]], device='cuda:0'), tensor([[0.6628]], device='cuda:0'), tensor([[1.5438]], device='cuda:0'), tensor([[0.0640]], device='cuda:0'), tensor([[0.3821]], device='cuda:0'), tensor([[0.1148]], device='cuda:0'), tensor([[-0.0098]], device='cuda:0'), tensor([[0.4537]], device='cuda:0'), tensor([[1.6697]], device='cuda:0'), tensor([[-0.2771]], device='cuda:0'), tensor([[0.4664]], device='cuda:0'), tensor([[0.7155]], device='cuda:0'), tensor([[1.9301]], device='cuda:0'), tensor([[1.3238]], device='cuda:0'), tensor([[1.6494]], device='cuda:0'), tensor([[0.2569]], device='cuda:0'), tensor([[0.1795]], device='cuda:0'), tensor([[1.9922]], device='cuda:0'), tensor([[-0.1674]], device='cuda:0'), tensor([[1.0392]], device='cuda:0'), tensor([[0.2914]], device='cuda:0'), tensor([[1.1724]], device='cuda:0'), tensor([[1.0354]], device='cuda:0'), tensor([[0.0581]], device='cuda:0'), tensor([[2.0342]], device='cuda:0'), tensor([[0.7840]], device='cuda:0'), tensor([[-0.0014]], device='cuda:0'), tensor([[-0.0250]], device='cuda:0'), tensor([[0.5104]], device='cuda:0'), tensor([[0.0464]], device='cuda:0'), tensor([[-0.3014]], device='cuda:0'), tensor([[0.4448]], device='cuda:0'), tensor([[0.0031]], device='cuda:0'), tensor([[-0.1587]], device='cuda:0'), tensor([[-0.2935]], device='cuda:0'), tensor([[0.1175]], device='cuda:0'), tensor([[-0.4459]], device='cuda:0'), tensor([[-0.2995]], device='cuda:0'), tensor([[-0.3752]], device='cuda:0'), tensor([[-0.2272]], device='cuda:0'), tensor([[0.3674]], device='cuda:0'), tensor([[0.4668]], device='cuda:0'), tensor([[0.3308]], device='cuda:0'), tensor([[-0.0319]], device='cuda:0'), tensor([[-0.3743]], device='cuda:0'), tensor([[2.2160]], device='cuda:0'), tensor([[0.2023]], device='cuda:0'), tensor([[-0.0704]], device='cuda:0'), tensor([[0.8044]], device='cuda:0'), tensor([[-0.2359]], device='cuda:0'), tensor([[0.7836]], device='cuda:0'), tensor([[0.7836]], device='cuda:0'), tensor([[0.2217]], device='cuda:0'), tensor([[0.2217]], device='cuda:0'), tensor([[0.6182]], device='cuda:0'), tensor([[0.8315]], device='cuda:0'), tensor([[0.8826]], device='cuda:0'), tensor([[1.0330]], device='cuda:0'), tensor([[0.0103]], device='cuda:0'), tensor([[-0.2716]], device='cuda:0'), tensor([[1.2704]], device='cuda:0'), tensor([[1.2704]], device='cuda:0'), tensor([[0.6378]], device='cuda:0'), tensor([[-0.2598]], device='cuda:0'), tensor([[0.4724]], device='cuda:0'), tensor([[0.4724]], device='cuda:0'), tensor([[0.1348]], device='cuda:0'), tensor([[0.4845]], device='cuda:0'), tensor([[0.2274]], device='cuda:0'), tensor([[0.2274]], device='cuda:0'), tensor([[0.2509]], device='cuda:0'), tensor([[0.1470]], device='cuda:0'), tensor([[3.2555]], device='cuda:0'), tensor([[-0.1745]], device='cuda:0'), tensor([[0.1940]], device='cuda:0'), tensor([[0.1940]], device='cuda:0'), tensor([[1.5942]], device='cuda:0'), tensor([[0.3732]], device='cuda:0'), tensor([[0.4941]], device='cuda:0'), tensor([[0.7616]], device='cuda:0'), tensor([[1.8654]], device='cuda:0'), tensor([[0.2028]], device='cuda:0'), tensor([[-0.2094]], device='cuda:0'), tensor([[2.5288]], device='cuda:0'), tensor([[0.5207]], device='cuda:0'), tensor([[0.5207]], device='cuda:0'), tensor([[2.0998]], device='cuda:0'), tensor([[-0.1811]], device='cuda:0'), tensor([[1.1215]], device='cuda:0'), tensor([[1.1215]], device='cuda:0'), tensor([[-0.1188]], device='cuda:0'), tensor([[-0.1188]], device='cuda:0'), tensor([[0.9152]], device='cuda:0'), tensor([[0.9152]], device='cuda:0'), tensor([[1.0557]], device='cuda:0'), tensor([[0.7395]], device='cuda:0'), tensor([[1.1003]], device='cuda:0'), tensor([[2.0362]], device='cuda:0'), tensor([[0.6901]], device='cuda:0'), tensor([[0.6901]], device='cuda:0'), tensor([[1.4013]], device='cuda:0'), tensor([[0.9535]], device='cuda:0'), tensor([[1.2106]], device='cuda:0'), tensor([[0.7766]], device='cuda:0'), tensor([[0.7766]], device='cuda:0'), tensor([[0.9634]], device='cuda:0'), tensor([[0.9634]], device='cuda:0'), tensor([[0.2882]], device='cuda:0'), tensor([[0.2882]], device='cuda:0'), tensor([[0.4439]], device='cuda:0'), tensor([[0.3334]], device='cuda:0'), tensor([[0.3334]], device='cuda:0'), tensor([[0.3529]], device='cuda:0'), tensor([[0.3529]], device='cuda:0'), tensor([[1.4651]], device='cuda:0'), tensor([[-0.0487]], device='cuda:0'), tensor([[0.6103]], device='cuda:0'), tensor([[0.6103]], device='cuda:0'), tensor([[0.8283]], device='cuda:0'), tensor([[0.8283]], device='cuda:0'), tensor([[-0.0967]], device='cuda:0'), tensor([[-0.0967]], device='cuda:0')]
# [0.5068403482437134, 0.5099735260009766, 0.8423260450363159, 0.8373044729232788, 0.6645318865776062, 0.8163273930549622, 0.8403652906417847, 0.8211761116981506, 0.9289568662643433, 0.8929368853569031, 0.7875257730484009, 0.821380078792572, 0.6417842507362366, 0.8395201563835144, 0.8445218205451965, 0.8294609189033508, 0.8158860802650452, 0.9281821846961975, 0.8177874684333801, 0.8634783029556274, 0.9375929236412048, 0.7626066207885742, 0.5114200711250305, 0.6588371992111206, 0.6596075892448425, 0.6993247866630554, 0.6913939118385315, 0.7640397548675537, 0.8573602437973022, 0.5372254848480225, 0.5372254848480225, 0.6084535717964172, 0.6211349964141846, 0.6211349964141846, 0.5048800706863403, 0.7692946195602417, 0.7339286804199219, 0.8897499442100525, 0.782059371471405, 0.7758336067199707, 0.6408990025520325, 0.8879532217979431, 0.7595744729042053, 0.8100815415382385, 0.7544275522232056, 0.9216422438621521, 0.919384241104126, 0.5575777888298035, 0.6908960342407227, 0.8628536462783813, 0.8910308480262756, 0.6253504753112793, 0.7821805477142334, 0.6239256262779236, 0.6704437136650085, 0.8570135235786438, 0.9155282378196716, 0.8389816880226135, 0.7921645045280457, 0.770140528678894, 0.7394739985466003, 0.8346550464630127, 0.6853930950164795, 0.7882305383682251, 0.8650528788566589, 0.6303066611289978, 0.6836256980895996, 0.7665585279464722, 0.8000453114509583, 0.9187096953392029, 0.8131998181343079, 0.8548210859298706, 0.7712520956993103, 0.8998678922653198, 0.5208761692047119, 0.7809785604476929, 0.8417403697967529, 0.9138820171356201, 0.885762631893158, 0.8124098181724548, 0.8463079929351807, 0.8003721833229065, 0.8003721833229065, 0.8818346261978149, 0.8818346261978149, 0.626122236251831, 0.626122236251831, 0.7571106553077698, 0.6626494526863098, 0.7777020931243896, 0.7777020931243896, 0.5475581884384155, 0.6960740089416504, 0.9584379196166992, 0.7917921543121338, 0.6793859601020813, 0.7185071110725403, 0.7068922519683838, 0.9144372344017029, 0.58698970079422, 0.6441265344619751, 0.6461613178253174, 0.7444706559181213, 0.8492574691772461, 0.5942507982254028, 0.7218289375305176, 0.6423371434211731, 0.8661297559738159, 0.628324568271637, 0.853962242603302, 0.853962242603302, 0.7573993802070618, 0.7556027173995972, 0.5905264616012573, 0.8669168949127197, 0.9213093519210815, 0.6391382217407227, 0.5392784476280212, 0.7407745718955994, 0.7606295943260193, 0.8716004490852356, 0.8889464139938354, 0.9253031015396118, 0.8743745684623718, 0.8673564791679382, 0.8998607397079468, 0.8012615442276001, 0.7196441888809204, 0.7891879081726074, 0.8692209720611572, 0.8365861177444458, 0.7645263075828552, 0.8501352667808533, 0.674656093120575, 0.7685278058052063, 0.6667748093605042, 0.6824147701263428, 0.9218693971633911, 0.9121823906898499, 0.7834106683731079, 0.8481760621070862, 0.7979575991630554, 0.5677838921546936, 0.8611750602722168, 0.671898365020752, 0.6733107566833496, 0.7731910347938538, 0.8431652188301086, 0.7462175488471985, 0.6568982601165771, 0.8527958989143372, 0.8035888075828552, 0.7286372780799866, 0.8475101590156555, 0.526190459728241, 0.8236212730407715, 0.6060842871665955, 0.6060842871665955, 0.7820059061050415, 0.6440097689628601, 0.8639850616455078, 0.5563300848007202, 0.781603217124939, 0.664584755897522, 0.7173330187797546, 0.7838624715805054, 0.7415057420730591, 0.5825334191322327, 0.8243131637573242, 0.5180866718292236, 0.778274655342102, 0.778274655342102, 0.7664011120796204, 0.6384989619255066, 0.7712072730064392, 0.8036717176437378, 0.7155009508132935, 0.802653968334198, 0.755305826663971, 0.8393154740333557, 0.563325822353363, 0.7352805137634277, 0.5501040816307068, 0.8649659752845764, 0.7578269839286804, 0.7618323564529419, 0.6991186738014221, 0.7150728106498718, 0.7563835978507996, 0.7681613564491272, 0.6305040717124939, 0.7819329500198364, 0.5375559329986572, 0.669765830039978, 0.8147748112678528, 0.7668173313140869, 0.7668173313140869, 0.9312328696250916, 0.5349004864692688, 0.8402737379074097, 0.5276453495025635, 0.7823775410652161, 0.6071934103965759, 0.8622536659240723, 0.6640365719795227, 0.595184862613678, 0.6887061595916748, 0.5324495434761047, 0.8562033772468567, 0.7920600771903992, 0.8767082691192627, 0.8041822910308838, 0.5426486730575562, 0.821145236492157, 0.5936039090156555, 0.820787787437439, 0.559406042098999, 0.8364658951759338, 0.6400588154792786, 0.5234313607215881, 0.7548311352729797, 0.6247856020927429, 0.6676455140113831, 0.6519384384155273, 0.5668410658836365, 0.7419676780700684, 0.5929965972900391, 0.6814404129981995, 0.6394529342651367, 0.7966070175170898, 0.7807669043540955, 0.7281925678253174, 0.9220531582832336, 0.5879812240600586, 0.8367648720741272, 0.7396202087402344, 0.822507917881012, 0.7879354357719421, 0.8478822112083435, 0.9488163590431213, 0.7824723720550537, 0.8968284726142883, 0.817988932132721, 0.7550497055053711, 0.8720952868461609, 0.7314296960830688, 0.9092108011245728, 0.8581768274307251, 0.8345831632614136, 0.9016597867012024, 0.7835128307342529, 0.8695422410964966, 0.6958702206611633, 0.6958702206611633, 0.8586326837539673, 0.8586326837539673, 0.6119721531867981, 0.5548897981643677, 0.5944847464561462, 0.5286287665367126, 0.7036308646202087, 0.7894580960273743, 0.60392826795578, 0.60392826795578, 0.6472427845001221, 0.7562840580940247, 0.8893733620643616, 0.8893733620643616, 0.8686441779136658, 0.6315630674362183, 0.7803183794021606, 0.7803183794021606, 0.7852527499198914, 0.77440345287323, 0.5955238342285156, 0.6599173545837402, 0.8791654109954834, 0.8791654109954834, 0.7126117944717407, 0.615363359451294, 0.8636882305145264, 0.6702619194984436, 0.6322711110115051, 0.7495912909507751, 0.8437486290931702, 0.5064924955368042, 0.6638666391372681, 0.6638666391372681, 0.5015842318534851, 0.8305150270462036, 0.637202799320221, 0.637202799320221, 0.7817095518112183, 0.7817095518112183, 0.5287032127380371, 0.5287032127380371, 0.8460094928741455, 0.6391868591308594, 0.6648460030555725, 0.7094239592552185, 0.6381515264511108, 0.6381515264511108, 0.6278660893440247, 0.6666162610054016, 0.6784599423408508, 0.7075289487838745, 0.7075289487838745, 0.7680037617683411, 0.7680037617683411, 0.9310546517372131, 0.9310546517372131, 0.7564916014671326, 0.774678111076355, 0.774678111076355, 0.8737087845802307, 0.8737087845802307, 0.8173511624336243, 0.951375424861908, 0.8427817821502686, 0.8427817821502686, 0.6420097351074219, 0.6420097351074219, 0.9129671454429626, 0.9129671454429626]
