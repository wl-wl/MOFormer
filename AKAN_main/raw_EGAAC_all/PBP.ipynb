{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-17T05:37:43.915747Z",
     "start_time": "2024-08-17T05:37:43.913290Z"
    }
   },
   "outputs": [],
   "source": [
    "from kan import KAN\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "import numpy as np\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('/tmp/pycharm_project_763/data/trainCPP.csv')\n",
    "# \n",
    "# # 提取序列和标签\n",
    "# sequences = df['sequence'].tolist()\n",
    "# y = df['label'].tolist()\n",
    "# \n",
    "# df2= pd.read_csv('/tmp/pycharm_project_763/data/testCPP.csv')\n",
    "# \n",
    "# # 提取序列和标签\n",
    "# sequences_test = df2['sequence'].tolist()\n",
    "# y_test = df2['label'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T05:13:23.688794Z",
     "start_time": "2024-08-17T05:13:23.664861Z"
    }
   },
   "id": "fca4be2405677df0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "('GWTLNSAGYLLGKINLKALAALAKKIL', 1, 'GKKKKKKKKK', int)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequences[0],y[0],sequences_test[0], type(y_test[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T05:14:19.278485Z",
     "start_time": "2024-08-17T05:14:19.268916Z"
    }
   },
   "id": "6b4fae4b2e02c053"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "def read_fasta(file_path):\n",
    "    protein_list=[]\n",
    "    label_list=[]\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                # 提取标签\n",
    "                label = line.split('|')[1]\n",
    "                label_list.append(int(label))\n",
    "            else:\n",
    "                # 提取蛋白质序列\n",
    "                protein_list.append(line)\n",
    "\n",
    "    return protein_list, label_list\n",
    "\n",
    "# 示例文件路径\n",
    "train_path = '/tmp/pycharm_project_763/raw/PBP.txt'\n",
    "test_path ='/tmp/pycharm_project_763/raw/PBPT.txt'\n",
    "\n",
    "sequences, y = read_fasta(train_path)\n",
    "sequences_test, y_test = read_fasta(test_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:07:57.020094Z",
     "start_time": "2024-08-17T06:07:57.013544Z"
    }
   },
   "id": "b7a975eecf9cf87e"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "('KWKLFKKIEKVGQNIRDGIIKAGPAVAVVGQATQIAK',\n 0,\n 'KWKLFKKIEKVGQNIRDGIIKAGPAVAVVGQATQIAK',\n 0)"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0],y[0],sequences_test[0], y_test[0] "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:07:58.076917Z",
     "start_time": "2024-08-17T06:07:58.068945Z"
    }
   },
   "id": "914c3d8d88af6eca"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "def EGAAC(sequences, window=5):\n",
    "    if window < 1:\n",
    "        print('Error: the sliding window should be greater than zero' + '\\n\\n')\n",
    "        return None\n",
    "\n",
    "    group = {\n",
    "        'alphatic': 'GAVLMI',\n",
    "        'aromatic': 'FYW',\n",
    "        'positive_charge': 'KRH',\n",
    "        'negative_charge': 'DE',\n",
    "        'uncharged': 'STCPNQ'\n",
    "    }\n",
    "\n",
    "    groupKeys = group.keys()\n",
    "\n",
    "    encodings = []\n",
    "    header = ['#']\n",
    "    max_len = max(len(seq) for seq in sequences)  # Find the maximum length of sequences\n",
    "    for w in range(1, max_len - window + 2):\n",
    "        for g in groupKeys:\n",
    "            header.append('SW.' + str(w) + '.' + g)\n",
    "    # encodings.append(header)\n",
    "\n",
    "    for sequence in sequences:\n",
    "        code = []\n",
    "        for j in range(len(sequence) - window + 1):\n",
    "            subseq = sequence[j:j + window]\n",
    "            count = Counter(subseq)\n",
    "            myDict = {}\n",
    "            for key in groupKeys:\n",
    "                myDict[key] = sum(count[aa] for aa in group[key] if aa in count)\n",
    "            for key in groupKeys:\n",
    "                code.append(myDict[key] / window)\n",
    "        encodings.append(code)\n",
    "\n",
    "    return encodings\n",
    "\n",
    "# 示例蛋白质序列列表\n",
    "\n",
    "EGAAC_train = EGAAC(sequences, window=5)\n",
    "max_length = max(len(item) for item in EGAAC_train)\n",
    "\n",
    "# 进行零填充\n",
    "train_encodings = np.array([np.pad(item, (0, max_length - len(item)), 'constant', constant_values=(0)) for item in EGAAC_train])\n",
    "# train_encodings\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:07:58.878092Z",
     "start_time": "2024-08-17T06:07:58.787104Z"
    }
   },
   "id": "53942c9b85a34800"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "EGAAC_test = EGAAC(sequences_test, window=5)\n",
    "# max_length = max(len(item) for item in EGAAC_test)\n",
    "\n",
    "# 进行零填充\n",
    "test_encodings = np.array([np.pad(item, (0, max_length - len(item)), 'constant', constant_values=(0)) for item in EGAAC_test])\n",
    "# test_encodings\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:07:59.743962Z",
     "start_time": "2024-08-17T06:07:59.687793Z"
    }
   },
   "id": "ebc0c79f4bac0954"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(train_encodings, y, random_state=42)\n",
    "X_test, y_test = shuffle(test_encodings, y_test, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:00.739913Z",
     "start_time": "2024-08-17T06:08:00.734532Z"
    }
   },
   "id": "f4de8086c0ea1557"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:01.383453Z",
     "start_time": "2024-08-17T06:08:01.374368Z"
    }
   },
   "id": "dcff175659aa7564"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train).view(-1, 1)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test).view(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:02.196322Z",
     "start_time": "2024-08-17T06:08:02.153127Z"
    }
   },
   "id": "11d5832ca8d162f9"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangli/anaconda3/envs/transformer-env/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/wangli/anaconda3/envs/transformer-env/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/wangli/anaconda3/envs/transformer-env/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/wangli/anaconda3/envs/transformer-env/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train ,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:02.722105Z",
     "start_time": "2024-08-17T06:08:02.710261Z"
    }
   },
   "id": "7801e22c6b38d586"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([500, 465]), torch.Size([532, 465]))"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:03.505037Z",
     "start_time": "2024-08-17T06:08:03.499416Z"
    }
   },
   "id": "5e86c99bd06b43e4"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0.]), torch.Size([532, 1]))"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0],y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:04.943618Z",
     "start_time": "2024-08-17T06:08:04.936695Z"
    }
   },
   "id": "b102765e7a2bc4da"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "dataset = {\n",
    "    'train_input': X_train,\n",
    "    'test_input': X_test,\n",
    "    'train_label': y_train,\n",
    "    'test_label': y_test\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:05.542716Z",
     "start_time": "2024-08-17T06:08:05.538763Z"
    }
   },
   "id": "67ee78003569c3e8"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    }
   ],
   "source": [
    "model = KAN(width=[230,5,1], grid=3, k=2, seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:06.275766Z",
     "start_time": "2024-08-17T06:08:06.236176Z"
    }
   },
   "id": "c92d88ba1dde4f86"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.20e-01 | test_loss: 4.99e-01 | reg: 0.00e+00 | :  20%|▏| 1/5 [00:02<00:10,  2.56s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9403, SP: 0.8760, MCC: 0.7361, SN: 0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.06e-01 | test_loss: 5.06e-01 | reg: 0.00e+00 | :  40%|▍| 2/5 [00:04<00:06,  2.12s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9911, SP: 0.9480, MCC: 0.9040, SN: 0.9560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.48e-01 | test_loss: 5.02e-01 | reg: 0.00e+00 | :  60%|▌| 3/5 [00:06<00:03,  1.98s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9955, SP: 0.9760, MCC: 0.9600, SN: 0.9840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.17e-01 | test_loss: 5.13e-01 | reg: 0.00e+00 | :  80%|▊| 4/5 [00:08<00:02,  2.06s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9988, SP: 0.9800, MCC: 0.9680, SN: 0.9880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.01e-01 | test_loss: 5.19e-01 | reg: 6.00e+01 | : 100%|█| 5/5 [00:12<00:00,  2.56s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9996, SP: 0.9880, MCC: 0.9760, SN: 0.9880\n",
      "saving model version 0.1\n",
      "0.9879999756813049 0.7218044996261597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "def train_acc():\n",
    "    pred=model(X_train)\n",
    "    with torch.no_grad():\n",
    "        pred_labels = (pred > 0.5).float() \n",
    "        auc = roc_auc_score(y_train.cpu(), pred.cpu())\n",
    "\n",
    "    # 混淆矩阵计算\n",
    "        tn, fp, fn, tp = confusion_matrix(y_train.cpu(), pred_labels.cpu()).ravel()\n",
    "\n",
    "        # Specificity (SP)\n",
    "        sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "        # Sensitivity (SN)\n",
    "        sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        # Matthews Correlation Coefficient (MCC)\n",
    "        mcc = matthews_corrcoef(y_train.cpu(), pred_labels.cpu())\n",
    "    print(f\"AUC: {auc:.4f}, SP: {sp:.4f}, MCC: {mcc:.4f}, SN: {sn:.4f}\")\n",
    "    return torch.mean((torch.round(model(X_train)[:, 0]) == y_train[:, 0]).float())\n",
    "\n",
    "def test_acc():\n",
    "    return torch.mean((torch.round(model(X_test)[:, 0]) == y_test[:, 0]).float())\n",
    "\n",
    "# results = model.train(dataset, opt=\"LBFGS\", steps=10, metrics=(train_acc, test_acc)) ,lamb=0.001 lamb_entropy=4.,lamb=0.1,lamb_l1=2.5,\n",
    "# lamb=0.005 train/fit\n",
    "results = model.fit(dataset, opt=\"LBFGS\", steps=5, metrics=(train_acc, test_acc));\n",
    "print(results['train_acc'][-1], results['test_acc'][-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:20.014568Z",
     "start_time": "2024-08-17T06:08:07.175874Z"
    }
   },
   "id": "76db8c058c2b606e"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangli/anaconda3/envs/transformer-env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69532209\n",
      "Iteration 2, loss = 0.64770278\n",
      "Iteration 3, loss = 0.60567541\n",
      "Iteration 4, loss = 0.57242321\n",
      "Iteration 5, loss = 0.55136494\n",
      "Iteration 6, loss = 0.52428361\n",
      "Iteration 7, loss = 0.50328327\n",
      "Iteration 8, loss = 0.48610853\n",
      "Iteration 9, loss = 0.46990645\n",
      "Iteration 10, loss = 0.45595826\n",
      "Iteration 11, loss = 0.44498660\n",
      "Iteration 12, loss = 0.43319008\n",
      "Iteration 13, loss = 0.42012422\n",
      "Iteration 14, loss = 0.41095926\n",
      "Iteration 15, loss = 0.40296285\n",
      "Iteration 16, loss = 0.39159482\n",
      "Iteration 17, loss = 0.38151331\n",
      "Iteration 18, loss = 0.37473407\n",
      "Iteration 19, loss = 0.36558455\n",
      "Iteration 20, loss = 0.35906342\n",
      "Iteration 21, loss = 0.35236798\n",
      "Iteration 22, loss = 0.34467775\n",
      "Iteration 23, loss = 0.33971679\n",
      "Iteration 24, loss = 0.33577404\n",
      "Iteration 25, loss = 0.33315645\n",
      "Iteration 26, loss = 0.32138853\n",
      "Iteration 27, loss = 0.31811865\n",
      "Iteration 28, loss = 0.30745227\n",
      "Iteration 29, loss = 0.30573118\n",
      "Iteration 30, loss = 0.30011910\n",
      "Iteration 31, loss = 0.29113946\n",
      "Iteration 32, loss = 0.28774158\n",
      "Iteration 33, loss = 0.28341764\n",
      "Iteration 34, loss = 0.28274895\n",
      "Iteration 35, loss = 0.27249249\n",
      "Iteration 36, loss = 0.26528984\n",
      "Iteration 37, loss = 0.26163164\n",
      "Iteration 38, loss = 0.25924832\n",
      "Iteration 39, loss = 0.25518708\n",
      "Iteration 40, loss = 0.24893107\n",
      "Iteration 41, loss = 0.24431066\n",
      "Iteration 42, loss = 0.24123963\n",
      "Iteration 43, loss = 0.23760552\n",
      "Iteration 44, loss = 0.23197756\n",
      "Iteration 45, loss = 0.22766248\n",
      "Iteration 46, loss = 0.22333406\n",
      "Iteration 47, loss = 0.21864338\n",
      "Iteration 48, loss = 0.21870339\n",
      "Iteration 49, loss = 0.21191415\n",
      "Iteration 50, loss = 0.20928372\n",
      "Iteration 51, loss = 0.20575272\n",
      "Iteration 52, loss = 0.20088447\n",
      "Iteration 53, loss = 0.19707827\n",
      "Iteration 54, loss = 0.19499377\n",
      "Iteration 55, loss = 0.19171730\n",
      "Iteration 56, loss = 0.19541357\n",
      "Iteration 57, loss = 0.18940944\n",
      "Iteration 58, loss = 0.18217766\n",
      "Iteration 59, loss = 0.18316585\n",
      "Iteration 60, loss = 0.17507633\n",
      "Iteration 61, loss = 0.17156957\n",
      "Iteration 62, loss = 0.17120723\n",
      "Iteration 63, loss = 0.16843614\n",
      "Iteration 64, loss = 0.18132320\n",
      "Iteration 65, loss = 0.16978322\n",
      "Iteration 66, loss = 0.15875758\n",
      "Iteration 67, loss = 0.15706728\n",
      "Iteration 68, loss = 0.15158065\n",
      "Iteration 69, loss = 0.15332740\n",
      "Iteration 70, loss = 0.14823253\n",
      "Iteration 71, loss = 0.14606075\n",
      "Iteration 72, loss = 0.14232504\n",
      "Iteration 73, loss = 0.14158703\n",
      "Iteration 74, loss = 0.15217482\n",
      "Iteration 75, loss = 0.14253012\n",
      "Iteration 76, loss = 0.14344065\n",
      "Iteration 77, loss = 0.13170111\n",
      "Iteration 78, loss = 0.12919221\n",
      "Iteration 79, loss = 0.12761633\n",
      "Iteration 80, loss = 0.12544066\n",
      "Iteration 81, loss = 0.12510054\n",
      "Iteration 82, loss = 0.12636093\n",
      "Iteration 83, loss = 0.11903838\n",
      "Iteration 84, loss = 0.11686308\n",
      "Iteration 85, loss = 0.11676866\n",
      "Iteration 86, loss = 0.11363146\n",
      "Iteration 87, loss = 0.11632969\n",
      "Iteration 88, loss = 0.11009578\n",
      "Iteration 89, loss = 0.11494033\n",
      "Iteration 90, loss = 0.11009086\n",
      "Iteration 91, loss = 0.11206369\n",
      "Iteration 92, loss = 0.10654701\n",
      "Iteration 93, loss = 0.10250332\n",
      "Iteration 94, loss = 0.10829710\n",
      "Iteration 95, loss = 0.10106432\n",
      "Iteration 96, loss = 0.09874402\n",
      "Iteration 97, loss = 0.10105047\n",
      "Iteration 98, loss = 0.09908679\n",
      "Iteration 99, loss = 0.09491116\n",
      "Iteration 100, loss = 0.09232012\n",
      "Iteration 101, loss = 0.09464117\n",
      "Iteration 102, loss = 0.08991102\n",
      "Iteration 103, loss = 0.08907212\n",
      "Iteration 104, loss = 0.09164569\n",
      "Iteration 105, loss = 0.08974460\n",
      "Iteration 106, loss = 0.08587628\n",
      "Iteration 107, loss = 0.08722697\n",
      "Iteration 108, loss = 0.08356993\n",
      "Iteration 109, loss = 0.09162261\n",
      "Iteration 110, loss = 0.08452821\n",
      "Iteration 111, loss = 0.08126161\n",
      "Iteration 112, loss = 0.08211250\n",
      "Iteration 113, loss = 0.07899130\n",
      "Iteration 114, loss = 0.07742044\n",
      "Iteration 115, loss = 0.07794521\n",
      "Iteration 116, loss = 0.07601568\n",
      "Iteration 117, loss = 0.07887481\n",
      "Iteration 118, loss = 0.07542124\n",
      "Iteration 119, loss = 0.07774958\n",
      "Iteration 120, loss = 0.07148624\n",
      "Iteration 121, loss = 0.07338067\n",
      "Iteration 122, loss = 0.06979713\n",
      "Iteration 123, loss = 0.06870356\n",
      "Iteration 124, loss = 0.07101499\n",
      "Iteration 125, loss = 0.06864943\n",
      "Iteration 126, loss = 0.06842089\n",
      "Iteration 127, loss = 0.07344338\n",
      "Iteration 128, loss = 0.06548382\n",
      "Iteration 129, loss = 0.06491886\n",
      "Iteration 130, loss = 0.06816241\n",
      "Iteration 131, loss = 0.06310842\n",
      "Iteration 132, loss = 0.06189076\n",
      "Iteration 133, loss = 0.06325761\n",
      "Iteration 134, loss = 0.07327139\n",
      "Iteration 135, loss = 0.06310224\n",
      "Iteration 136, loss = 0.06190600\n",
      "Iteration 137, loss = 0.05891202\n",
      "Iteration 138, loss = 0.05927705\n",
      "Iteration 139, loss = 0.05839692\n",
      "Iteration 140, loss = 0.05866684\n",
      "Iteration 141, loss = 0.06182184\n",
      "Iteration 142, loss = 0.05678946\n",
      "Iteration 143, loss = 0.05978380\n",
      "Iteration 144, loss = 0.06138905\n",
      "Iteration 145, loss = 0.05874544\n",
      "Iteration 146, loss = 0.05607656\n",
      "Iteration 147, loss = 0.05436668\n",
      "Iteration 148, loss = 0.05537036\n",
      "Iteration 149, loss = 0.05364240\n",
      "Iteration 150, loss = 0.05442192\n",
      "Iteration 151, loss = 0.05325592\n",
      "Iteration 152, loss = 0.05205510\n",
      "Iteration 153, loss = 0.05422437\n",
      "Iteration 154, loss = 0.05189635\n",
      "Iteration 155, loss = 0.05075253\n",
      "Iteration 156, loss = 0.05443907\n",
      "Iteration 157, loss = 0.04950099\n",
      "Iteration 158, loss = 0.05073472\n",
      "Iteration 159, loss = 0.04887441\n",
      "Iteration 160, loss = 0.04933449\n",
      "Iteration 161, loss = 0.04761306\n",
      "Iteration 162, loss = 0.04796223\n",
      "Iteration 163, loss = 0.04730290\n",
      "Iteration 164, loss = 0.04715574\n",
      "Iteration 165, loss = 0.04710960\n",
      "Iteration 166, loss = 0.04827793\n",
      "Iteration 167, loss = 0.04687535\n",
      "Iteration 168, loss = 0.04576998\n",
      "Iteration 169, loss = 0.04502438\n",
      "Iteration 170, loss = 0.04488613\n",
      "Iteration 171, loss = 0.04535966\n",
      "Iteration 172, loss = 0.04514046\n",
      "Iteration 173, loss = 0.04801914\n",
      "Iteration 174, loss = 0.05072703\n",
      "Iteration 175, loss = 0.04556262\n",
      "Iteration 176, loss = 0.04728272\n",
      "Iteration 177, loss = 0.04749691\n",
      "Iteration 178, loss = 0.04332951\n",
      "Iteration 179, loss = 0.04426215\n",
      "Iteration 180, loss = 0.04861824\n",
      "Iteration 181, loss = 0.04282209\n",
      "Iteration 182, loss = 0.04349473\n",
      "Iteration 183, loss = 0.04185287\n",
      "Iteration 184, loss = 0.04180913\n",
      "Iteration 185, loss = 0.04294732\n",
      "Iteration 186, loss = 0.04143462\n",
      "Iteration 187, loss = 0.04082031\n",
      "Iteration 188, loss = 0.04249684\n",
      "Iteration 189, loss = 0.04116508\n",
      "Iteration 190, loss = 0.04085671\n",
      "Iteration 191, loss = 0.04264573\n",
      "Iteration 192, loss = 0.04115876\n",
      "Iteration 193, loss = 0.04207841\n",
      "Iteration 194, loss = 0.04464907\n",
      "Iteration 195, loss = 0.04249910\n",
      "Iteration 196, loss = 0.04186184\n",
      "Iteration 197, loss = 0.04005513\n",
      "Iteration 198, loss = 0.04363400\n",
      "Iteration 199, loss = 0.04115569\n",
      "Iteration 200, loss = 0.03951018\n",
      "Iteration 201, loss = 0.03778985\n",
      "Iteration 202, loss = 0.03758124\n",
      "Iteration 203, loss = 0.03687697\n",
      "Iteration 204, loss = 0.03775982\n",
      "Iteration 205, loss = 0.03912395\n",
      "Iteration 206, loss = 0.03697985\n",
      "Iteration 207, loss = 0.03624964\n",
      "Iteration 208, loss = 0.03917146\n",
      "Iteration 209, loss = 0.03851458\n",
      "Iteration 210, loss = 0.03824461\n",
      "Iteration 211, loss = 0.04055964\n",
      "Iteration 212, loss = 0.03912256\n",
      "Iteration 213, loss = 0.03773563\n",
      "Iteration 214, loss = 0.03838004\n",
      "Iteration 215, loss = 0.03543129\n",
      "Iteration 216, loss = 0.03479328\n",
      "Iteration 217, loss = 0.03843558\n",
      "Iteration 218, loss = 0.03521258\n",
      "Iteration 219, loss = 0.03586157\n",
      "Iteration 220, loss = 0.03991034\n",
      "Iteration 221, loss = 0.03441345\n",
      "Iteration 222, loss = 0.03555410\n",
      "Iteration 223, loss = 0.03512486\n",
      "Iteration 224, loss = 0.03515928\n",
      "Iteration 225, loss = 0.03651027\n",
      "Iteration 226, loss = 0.03303529\n",
      "Iteration 227, loss = 0.03344212\n",
      "Iteration 228, loss = 0.03644913\n",
      "Iteration 229, loss = 0.03476070\n",
      "Iteration 230, loss = 0.03459538\n",
      "Iteration 231, loss = 0.03825765\n",
      "Iteration 232, loss = 0.03969855\n",
      "Iteration 233, loss = 0.03351021\n",
      "Iteration 234, loss = 0.03257420\n",
      "Iteration 235, loss = 0.03266976\n",
      "Iteration 236, loss = 0.03310138\n",
      "Iteration 237, loss = 0.03308951\n",
      "Iteration 238, loss = 0.03375788\n",
      "Iteration 239, loss = 0.03775609\n",
      "Iteration 240, loss = 0.03344668\n",
      "Iteration 241, loss = 0.03314589\n",
      "Iteration 242, loss = 0.03326303\n",
      "Iteration 243, loss = 0.03257137\n",
      "Iteration 244, loss = 0.03348484\n",
      "Iteration 245, loss = 0.03181074\n",
      "Iteration 246, loss = 0.03355124\n",
      "Iteration 247, loss = 0.03419604\n",
      "Iteration 248, loss = 0.03292034\n",
      "Iteration 249, loss = 0.03505894\n",
      "Iteration 250, loss = 0.03274264\n",
      "Iteration 251, loss = 0.03282230\n",
      "Iteration 252, loss = 0.03124988\n",
      "Iteration 253, loss = 0.03049527\n",
      "Iteration 254, loss = 0.03366591\n",
      "Iteration 255, loss = 0.03851971\n",
      "Iteration 256, loss = 0.03116447\n",
      "Iteration 257, loss = 0.03358377\n",
      "Iteration 258, loss = 0.03044991\n",
      "Iteration 259, loss = 0.03144349\n",
      "Iteration 260, loss = 0.03218276\n",
      "Iteration 261, loss = 0.03114526\n",
      "Iteration 262, loss = 0.03031343\n",
      "Iteration 263, loss = 0.03321702\n",
      "Iteration 264, loss = 0.02912291\n",
      "Iteration 265, loss = 0.02970806\n",
      "Iteration 266, loss = 0.03127869\n",
      "Iteration 267, loss = 0.03317791\n",
      "Iteration 268, loss = 0.03236824\n",
      "Iteration 269, loss = 0.02967484\n",
      "Iteration 270, loss = 0.02949482\n",
      "Iteration 271, loss = 0.02960183\n",
      "Iteration 272, loss = 0.02850484\n",
      "Iteration 273, loss = 0.02951556\n",
      "Iteration 274, loss = 0.03185911\n",
      "Iteration 275, loss = 0.02954130\n",
      "Iteration 276, loss = 0.02923779\n",
      "Iteration 277, loss = 0.02885985\n",
      "Iteration 278, loss = 0.02845054\n",
      "Iteration 279, loss = 0.02893641\n",
      "Iteration 280, loss = 0.02841017\n",
      "Iteration 281, loss = 0.02838966\n",
      "Iteration 282, loss = 0.02880340\n",
      "Iteration 283, loss = 0.02988211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": "MLPClassifier(learning_rate_init=0.1, max_iter=300, random_state=1,\n              solver='sgd', verbose=10)"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50,50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:26.005142Z",
     "start_time": "2024-08-17T06:08:24.179662Z"
    }
   },
   "id": "6ee8b621a5c4d943"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9920\n",
      "Test Accuracy: 0.7237\n"
     ]
    }
   ],
   "source": [
    "predictions_train = mlp.predict(X_train)\n",
    "predictions_test = mlp.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "train_accuracy = accuracy_score(y_train, predictions_train)\n",
    "test_accuracy = accuracy_score(y_test, predictions_test)\n",
    "\n",
    "print(f'Train Accuracy: {train_accuracy:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T06:08:29.935755Z",
     "start_time": "2024-08-17T06:08:29.892078Z"
    }
   },
   "id": "a1116721c574927c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4fdec9086f11ac96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
